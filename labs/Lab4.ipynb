{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bias, Variance, and Cross Validation\n",
    "\n",
    "We learned in the lecture about cross-validation with an explanation about finding the right hyper-parameters, some of which were regularization parameters. We will have more to say about regularization soon, but lets tackle the reasons we do cross-validation.\n",
    "\n",
    "The bottom line is: finding the model which has an appropriate mix of bias and variance. We usually want to sit at the point of the tradeoff between the two: be simple but no simpler than necessary.\n",
    "\n",
    "We do not want a model with too much variance: it would not generalize well. This phenomenon is also called *overfitting*. There is no point doing prediction if we cant generalize well. At the same time, if we have too much bias in our model, we will systematically underpredict or overpredict values and miss most predictions. This is also known as *underfitting*.\n",
    "\n",
    "Cross-Validation provides us a way to find the \"hyperparameters\" of our model, such that we achieve the balance point.\n",
    "\n",
    "Read http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import brewer2mpl\n",
    "from matplotlib import rcParams\n",
    "\n",
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_cmap = brewer2mpl.get_map('Dark2', 'Qualitative', 7)\n",
    "dark2_colors = dark2_cmap.mpl_colors\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "\n",
    "\n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Polyfit*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "def scatter_by(df, scatterx, scattery, by=None, figure=None, axes=None, colorscale=dark2_cmap, labeler={}, mfunc=None, setupfunc=None, mms=8):\n",
    "    cs=copy.deepcopy(colorscale.mpl_colors)\n",
    "    if not figure:\n",
    "        figure=plt.figure(figsize=(8,8))\n",
    "    if not axes:\n",
    "        axes=figure.gca()\n",
    "    x=df[scatterx]\n",
    "    y=df[scattery]\n",
    "    if not by:\n",
    "        col=random.choice(cs)\n",
    "        axes.scatter(x, y, cmap=colorscale, c=col)\n",
    "        if setupfunc:\n",
    "            axeslist=setupfunc(axes, figure)\n",
    "        else:\n",
    "            axeslist=[axes]\n",
    "        if mfunc:\n",
    "            mfunc(axeslist,x,y,color=col, mms=mms)\n",
    "    else:\n",
    "        cs=list(np.linspace(0,1,len(df.groupby(by))))\n",
    "        xlimsd={}\n",
    "        ylimsd={}\n",
    "        xs={}\n",
    "        ys={}\n",
    "        cold={}\n",
    "        for k,g in df.groupby(by):\n",
    "            col=cs.pop()\n",
    "            x=g[scatterx]\n",
    "            y=g[scattery]\n",
    "            xs[k]=x\n",
    "            ys[k]=y\n",
    "            c=colorscale.mpl_colormap(col)\n",
    "            cold[k]=c\n",
    "            axes.scatter(x, y, c=c, label=labeler.get(k,k), s=40, alpha=0.3);\n",
    "            xlimsd[k]=axes.get_xlim()\n",
    "            ylimsd[k]=axes.get_ylim()\n",
    "        xlims=[min([xlimsd[k][0] for k in xlimsd.keys()]), max([xlimsd[k][1] for k in xlimsd.keys()])]\n",
    "        ylims=[min([ylimsd[k][0] for k in ylimsd.keys()]), max([ylimsd[k][1] for k in ylimsd.keys()])]\n",
    "        axes.set_xlim(xlims)\n",
    "        axes.set_ylim(ylims)\n",
    "        if setupfunc:\n",
    "            axeslist=setupfunc(axes, figure)\n",
    "        else:\n",
    "            axeslist=[axes]\n",
    "        if mfunc:\n",
    "            for k in xs.keys():\n",
    "                mfunc(axeslist,xs[k],ys[k],color=cold[k], mms=mms);\n",
    "    axes.set_xlabel(scatterx);\n",
    "    axes.set_ylabel(scattery);\n",
    "    \n",
    "    return axes\n",
    "\n",
    "def make_rug(axeslist, x, y, color='b', mms=8):\n",
    "    axes=axeslist[0]\n",
    "    zerosx1=np.zeros(len(x))\n",
    "    zerosx2=np.zeros(len(x))\n",
    "    xlims=axes.get_xlim()\n",
    "    ylims=axes.get_ylim()\n",
    "    zerosx1.fill(ylims[1])\n",
    "    zerosx2.fill(xlims[1])\n",
    "    axes.plot(x, zerosx1, marker='|', color=color, ms=mms)\n",
    "    axes.plot(zerosx2, y, marker='_', color=color, ms=mms)\n",
    "    axes.set_xlim(xlims)\n",
    "    axes.set_ylim(ylims)\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any learning problem, or goal is **to minimize the prediction error on the test set**. This prediction error could be a root mean square error, or a 1-0 loss function, or a log likelyhood, or something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Polynomial regression\n",
    "\n",
    "This part of the lab is partly taken from\n",
    "\n",
    "http://raw.github.com/jakevdp/sklearn_pycon2013/master/notebooks/09_validation_and_testing.ipynb. Images are taken from Andrew Ng's Coursera course, on which the above notebook is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the model selection problem: what degree of polynomial you want to fit: `d`. It acts like a hyperparameter, in the sense that it is a second parameter that needs to be fit for. Once you set it, you still have to fit the parameters of your linear or polynomial or elsewise model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rmse(p,x,y):\n",
    "    yfit = np.polyval(p, x)\n",
    "    return np.sqrt(np.mean((y - yfit) ** 2))\n",
    "\n",
    "def generate_curve(x, sigma):\n",
    "    return np.random.normal(10 - 1. / (x + 0.1), sigma)\n",
    "x = 10 ** np.linspace(-2, 0, 8)\n",
    "intrinsic_error=1.\n",
    "y=generate_curve(x, intrinsic_error)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high bias situation is one in which we underfit. Notice how for low d, the `rmse` on the training set remains high. A high variance situation is one in which we overfit. We want to be just right. As we get to the limit of being able to interpolate the points, the rmse training error goes to nil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_new=np.linspace(-0.2, 1.2, 1000)\n",
    "plt.scatter(x,y, s=50)\n",
    "f1=np.polyfit(x,y,1)\n",
    "plt.plot(x_new,np.polyval(f1,x_new))\n",
    "print(\"d=1, rmse=\",rmse(f1,x,y))\n",
    "f2=np.polyfit(x,y,2)\n",
    "plt.plot(x_new,np.polyval(f2,x_new))\n",
    "print(\"d=2, rmse=\",rmse(f2,x,y))\n",
    "f4=np.polyfit(x,y,4)\n",
    "plt.plot(x_new,np.polyval(f4,x_new))\n",
    "print(\"d=4, rmse=\",rmse(f4,x,y))\n",
    "f6=np.polyfit(x,y,6)\n",
    "plt.plot(x_new,np.polyval(f6,x_new))\n",
    "print(\"d=6, rmse=\",rmse(f6,x,y))\n",
    "plt.xlim(-0.2, 1.2)\n",
    "plt.ylim(-1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves start taking on all kinds of wiggles so as to be able to fit themselves in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "x = np.random.random(N)\n",
    "y = generate_curve(x, intrinsic_error)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, train_size=0.6)\n",
    "plt.scatter(xtrain, ytrain, color='red')\n",
    "plt.scatter(xtest, ytest, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = np.arange(21)\n",
    "train_err = np.zeros(len(ds))\n",
    "test_err = np.zeros(len(ds))\n",
    "\n",
    "for i, d in enumerate(ds):\n",
    "    p = np.polyfit(xtrain, ytrain, d)\n",
    "\n",
    "    train_err[i] = rmse(p, xtrain, ytrain)\n",
    "    test_err[i] = rmse(p, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ds, test_err, lw=2, label = 'test error')\n",
    "ax.plot(ds, train_err, lw=2, label = 'training error')\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('degree of fit')\n",
    "ax.set_ylabel('rms error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to tell that a hypothesis is overfitting? Its not enough that the training error is low, though thats certainly an indication.\n",
    "\n",
    "The training error is low but test error is high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot training error against, say, d, the training error will decrease with increasing d. But for the cross-validation (or for that matter, test error), we'll have an error curve which has a minumum and goes up again.\n",
    "\n",
    "![polynomial regression](../data/bias-variance-error.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the word test and cv interchangeably here, but they really are not, as will be clear soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Learning Curves\n",
    "\n",
    "Here we plot the train vs cv/test error as a function of the size of the training set.\n",
    "\n",
    "The training set error increases as size of the data set increases. The intuition is that with more samples, you get further away from the interpolation limit. The cross validation error on the otherhand will decrease as **training set** size increases, as , more\n",
    "data you have better the hypothesis you fit.\n",
    "\n",
    "**High Bias**\n",
    "\n",
    "Now consider the high bias situation. The training error will increase as before, to a point, and then flatten out. (There is only so much you can do to make a straight line fit a quadratic curve). The cv/test error, on the other hand will decrease, but then, it too will flatten out. These will be very close to each other, and after a point, getting more training data will not help!\n",
    "\n",
    "![Learning Curve under high bias situation](../data/lc-hb.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#taken lock stock and barrel from Vanderplas.\n",
    "def plot_learning_curve(d):\n",
    "    sizes = np.linspace(2, N, 50).astype(int)\n",
    "    train_err = np.zeros(sizes.shape)\n",
    "    crossval_err = np.zeros(sizes.shape)\n",
    "\n",
    "    for i, size in enumerate(sizes):\n",
    "        # Train on only the first `size` points\n",
    "        p = np.polyfit(xtrain[:size], ytrain[:size], d)\n",
    "        \n",
    "        # Validation error is on the *entire* validation set\n",
    "        crossval_err[i] = rmse(p, xtest, ytest)\n",
    "        \n",
    "        # Training error is on only the points used for training\n",
    "        train_err[i] = rmse(p, xtrain[:size], ytrain[:size])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(sizes, crossval_err, lw=2, label='validation error')\n",
    "    ax.plot(sizes, train_err, lw=2, label='training error')\n",
    "    ax.plot([0, N], [intrinsic_error, intrinsic_error], '--k', label='intrinsic error')\n",
    "\n",
    "    ax.set_xlabel('training set size')\n",
    "    ax.set_ylabel('rms error')\n",
    "    \n",
    "    ax.legend(loc=0)\n",
    "    \n",
    "    ax.set_xlim(0, 99)\n",
    "\n",
    "    ax.set_title('d = %i' % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(d=1)\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point of balance the learning curves come together and carry on, close to the intrinsic error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(d=5)\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next consider the high variance situation. The training error will start out very low as usual, and go up slowly as even though we add points, we have enough wiggle room to start with, until it runs out and the error keeps increasing. The cv error, will, on the other hand, start out quite high, and remain high. Thus we will have a gap. In this case it will make sense to take more data, as that would drive the cv error down, and the training error up, until they meet.\n",
    "\n",
    "![Learning Curve under high variance situation](../data/lc-hv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(d=20)\n",
    "plt.ylim(0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

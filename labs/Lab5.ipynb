{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the cell below evaluates before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.interpolate import LSQUnivariateSpline\n",
    "from PIL import Image\n",
    "from IPython.display import Image as Im\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MeanShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation: The Right and Wrong Way (Lecture 6)\n",
    "\n",
    "As we mentioned in Lecture 6 it is not uncommon to see scientists use Cross Validation the wrong way. See [this blog post](http://followthedata.wordpress.com/2013/10/30/the-importance-of-proper-cross-validation-and-experimental-design/) about a study which claimed to have found a genetic signature for autism, which supposedly could be used for a screening test for autism risk at birth or in infancy. The authors had first performed feature selection on the whole data set and then  divided the data set into a training set and a validation set to assess the performance.\n",
    "\n",
    "So let us model this to see what happens.\n",
    "\n",
    "## The scenario\n",
    "\n",
    "You have 20 data points, each of which has 1,000,000 attributes. Each observation also has an associated $y$ value, and you are interested in whether a linear combination of a few attributes can be used to predict $y$. That is, you are looking for a model\n",
    "\n",
    "$$\n",
    "y_i \\sim \\sum_j \\beta_j x_{ij}\n",
    "$$\n",
    "\n",
    "where most of the 1 million $\\beta_j$ values are 0.\n",
    "\n",
    "## The problem\n",
    "\n",
    "Since there are so many more attributes than data points, the chance that a few attributes correlate with $y$ by pure coincidence is fairly high. \n",
    "\n",
    "If you think about it,  cross-validation helps you detect over-fitting, but you're perhaps fuzzy on the details.\n",
    "\n",
    "## The wrong way to cross-validate\n",
    "\n",
    "* Determine a few attributes of $X$ that correlate well with $Y$\n",
    "* Use cross-validation to measure how well a linear fit to these attributes predicts $y$\n",
    "\n",
    "Let's make the dataset, and compute the $y$'s with a \"hidden\" model that we are trying to recover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hidden_model(x):\n",
    "    #y is a linear combination of columns 5 and 10...\n",
    "    result = x[:, 5] + x[:, 10]\n",
    "    #... with a little noise\n",
    "    result += np.random.normal(0, .005, result.shape)\n",
    "    return result\n",
    "    \n",
    "def make_x(num_obs):\n",
    "    return np.random.uniform(0, 3, (num_obs, 10 ** 6))\n",
    "\n",
    "x = make_x(20)\n",
    "y = hidden_model(x)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to find the 2 attributes in $X$ that best correlate with $Y$. Recall from Lecture 7 we could do feature selection, and in fact the `sklearn.feature_selection.SelectKBest` will give us the `k` best features with the lowest $p$-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_regression, k=2).fit(x, y)\n",
    "best_features = np.where(selector.get_support())[0]\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we are already in trouble --- we've selected 2 columns which correlate with $Y$ by chance, but neither of which are columns 5 or 10 (the only 2 columns that *actually* have anything to do with $Y$). We can look at the correlations between these columns and $Y$, and confirm they are pretty good (again, just a coincidence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for b in best_features:\n",
    "    plt.plot(x[:, b], y, 'o')\n",
    "    plt.title(\"Column %i\" % b)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression on the full data with these two features looks good. The \"score\" here is the $R^2$ score --- scores close to 1 imply a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xt = x[:, best_features]\n",
    "clf = LinearRegression().fit(xt, y)\n",
    "print(\"Score is \", clf.score(xt, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yp = clf.predict(xt)\n",
    "plt.plot(yp, y, 'o')\n",
    "plt.plot(y, y, 'r-')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the model fits so well, we are worried about overfitting, but let's use cross-validation to detect this via `sklearn.cross_validation.cross_val_score`. Let's look at the average $R^2$ score, when performing 5-fold cross validation. It's not as good, but still not bad..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, xt, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even if we make a plot of the predicted and actual data at each cross-validation iteration, the model seems to predict the \"independent\" data pretty well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train, test in KFold(len(y), 10):\n",
    "    xtrain, xtest, ytrain, ytest = xt[train], xt[test], y[train], y[test]\n",
    "\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    yp = clf.predict(xtest)\n",
    "    \n",
    "    plt.plot(yp, ytest, 'o')\n",
    "    plt.plot(ytest, ytest, 'r-')\n",
    "    \n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But** --- what if we generated some more data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x2 = make_x(100)\n",
    "y2 = hidden_model(x2)\n",
    "x2 = x2[:, best_features]\n",
    "\n",
    "y2p = clf.predict(x2)\n",
    "\n",
    "plt.plot(y2p, y2, 'o')\n",
    "plt.plot(y2, y2, 'r-')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes --- there is no correlation at all! Cross-validation did **not** detect the overfitting, because we used the entire data to select \"good\" features before hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The right way to Cross-Validate\n",
    "\n",
    "To prevent overfitting, we can't let *any* information about the full dataset leak into cross-validation. Thus, we must re-select good features in each cross-validation iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for train, test in KFold(len(y), n_folds=5):\n",
    "    xtrain, xtest, ytrain, ytest = x[train], x[test], y[train], y[test]\n",
    "    \n",
    "    b = SelectKBest(f_regression, k=2)\n",
    "    b.fit(xtrain, ytrain)\n",
    "    xtrain = xtrain[:, b.get_support()]\n",
    "    xtest = xtest[:, b.get_support()]\n",
    "    \n",
    "    clf.fit(xtrain, ytrain)    \n",
    "    scores.append(clf.score(xtest, ytest))\n",
    "\n",
    "    yp = clf.predict(xtest)\n",
    "    plt.plot(yp, ytest, 'o')\n",
    "    plt.plot(ytest, ytest, 'r-')\n",
    "    \n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Observed\")\n",
    "\n",
    "print(\"CV Score is \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now cross-validation properly detects overfitting, by reporting a low average $R^2$ score and a plot that looks like noise. Of course, it doesn't help us actually discover the fact that columns 5 and 10 determine $Y$ (this task is probably hopeless without more data) --- it just lets us know when our fitting approach isn't generalizing to new data.\n",
    "\n",
    "---\n",
    "\n",
    "# Feature Selection (Lecture 7)\n",
    "\n",
    "Here we apply the best subset selection approach to the `Hitters` data. We wish to predict a baseball player’s `Salary` on the basis of various statistics associated with performance in the previous year. We drop any missing rows in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hitters_df = pd.read_csv(\"../data/Hitters.csv\", index_col ='Name')\n",
    "hitters_df.dropna(inplace=True)\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a regression problem, we first need to convert the non-numeric input variables to factors. We will use the `pd.factorize` here to encode the categorical features as dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hitters_df[\"League\"] = pd.factorize(hitters_df[\"League\"])[0]\n",
    "hitters_df[\"Division\"] = pd.factorize(hitters_df[\"Division\"])[0]\n",
    "hitters_df[\"NewLeague\"] = pd.factorize(hitters_df[\"NewLeague\"])[0]\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a baseline regressor with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collist = [col for col in hitters_df.columns if col != \"Salary\"]\n",
    "X = hitters_df[collist]\n",
    "y = hitters_df[\"Salary\"]\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "ypred = reg.predict(X)\n",
    "base_mse = np.sqrt(mean_squared_error(ypred, y))\n",
    "print(\"Baseline Training MSE is \", base_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Regression\n",
    "\n",
    "Use `SelectKBest` to plot the mean square error for each `k` in `n_features = range(1, len(collist))`. We use the `get_support()` function to get the index of the features selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mses = []\n",
    "n_features = range(1, len(collist))\n",
    "for k in n_features:\n",
    "    # compute MSE for different values of k (top features)\n",
    "    selector = SelectKBest(f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    selected = selector.get_support()\n",
    "    feats = [col for (col,sel) in zip(collist, selected) if sel]\n",
    "    reg = LinearRegression()\n",
    "    X_r = hitters_df[feats]\n",
    "    reg.fit(X_r, y)\n",
    "    ypred = reg.predict(X_r)\n",
    "    mses.append(np.sqrt(mean_squared_error(ypred, y)))\n",
    "\n",
    "plt.plot(n_features, mses)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection by Cross-Validation (Your Turn)\n",
    "\n",
    "The RMSE falls as the number of features increase --- this is expected because we are computing the RMSE from the training set (overfitting). We will now use 10-fold cross validation on each model to calculate a cross-validation RMSE which will give us a better idea of the best feature size to use for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_errors = []\n",
    "kfold = KFold(len(hitters_df), n_folds=10)\n",
    "n_features = range(1, len(collist))\n",
    "for k in n_features:\n",
    "    # build model with varying number of features\n",
    "    selector = SelectKBest(f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    selected = selector.get_support()\n",
    "    feats = [col for (col,sel) in zip(collist, selected) if sel]\n",
    "    \n",
    "    # and then use the train and test in kfold to calculate CV MSE:\n",
    "    rmses = []\n",
    "    for train, test in kfold:\n",
    "        # each model is cross validated 10 times\n",
    "        Xtrain, ytrain, Xtest, ytest = X[train], y[train], X[test], y[test]\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(Xtrain, ytrain)\n",
    "        ypred = reg.predict(Xtest)\n",
    "        rmses.append(np.sqrt(mean_squared_error(ytest, ypred)))\n",
    "        \n",
    "    # CV MSE is the mean of the CV rmses:\n",
    "    cv_errors.append(np.mean(rmses))\n",
    "plt.plot(n_features, cv_errors)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression and the Lasso (Your Turn)\n",
    "\n",
    "Recall that these methods improve the model by shrinking the coefficients. The difference was in the norm used as the second constraint. Ridge regression used $\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}$ where $\\lambda \\ge 0$, so a L2 norm. Lasso used $ \\lambda \\sum_{j=1}^{p} |\\beta_{j}|$ as the penalty and hence an L1 norm.\n",
    "\n",
    "Here we use cross validation to compute the RMSE for a baseline model, a model regularized by Ridge regression and one regularized using Lasso. Note the regularisation parameter $\\lambda$ is called `alpha` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validate(X, y, nfolds, reg_name):\n",
    "    rmses = []\n",
    "    kfold = KFold(X.shape[0], n_folds=nfolds)\n",
    "    for train, test in kfold:\n",
    "        Xtrain, ytrain, Xtest, ytest = X[train], y[train], X[test], y[test]\n",
    "        reg = None\n",
    "        if reg_name == \"ridge\":\n",
    "            reg = Ridge()\n",
    "        elif reg_name == \"lasso\":\n",
    "            reg = Lasso()\n",
    "        else:\n",
    "            reg = LinearRegression()\n",
    "        reg.fit(Xtrain, ytrain)\n",
    "        ypred = reg.predict(Xtest)\n",
    "        rmses.append(np.sqrt(mean_squared_error(ytest, ypred)))\n",
    "    return np.mean(rmses)\n",
    "\n",
    "rmse_baseline = cross_validate(X.values, y.values, 10, \"baseline\")\n",
    "rmse_ridge = cross_validate(X.values, y.values, 10, \"ridge\")\n",
    "rmse_lasso = cross_validate(X.values, y.values, 10, \"lasso\")\n",
    "(rmse_baseline, rmse_ridge, rmse_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an optimum value of `alpha` for the Lasso regressor using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_errors = []\n",
    "alphas = [0.1 * alpha for alpha in range(1, 250, 20)]\n",
    "kfold = KFold(X.shape[0], n_folds=10)\n",
    "for alpha in alphas:\n",
    "    rmses = []\n",
    "    for train, test in kfold:\n",
    "        Xtrain, ytrain, Xtest, ytest = X.values[train], y.values[train], X.values[test], y.values[test]\n",
    "        reg = Lasso(alpha=alpha)\n",
    "        reg.fit(Xtrain, ytrain)\n",
    "        ypred = reg.predict(Xtest)\n",
    "        rmses.append(np.sqrt(mean_squared_error(ytest, ypred)))\n",
    "        \n",
    "    cv_errors.append(np.mean(rmses))\n",
    "plt.plot(alphas, cv_errors)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the Lasso coefficients as a function of the regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = [0.1 * alpha for alpha in range(1, 250, 20)]\n",
    "reg = Lasso()\n",
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    reg.set_params(alpha=alpha)\n",
    "    reg.fit(X,y)\n",
    "    coefs.append(reg.coef_)\n",
    "\n",
    "plt.plot(alphas, coefs)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dimension Reduction: PCA (Lecture 7)\n",
    "\n",
    "Recall PCA from Lecture 7 where we project the data onto a lower-dimensional space and use that as our new features.  The principle behind the projections is captured below in this plot from stack overflow:\n",
    "\n",
    "![](../data/pcavsfit.png)\n",
    "\n",
    "We will use PCA to distinguish between cash and check at an ATM. We have some bolierplate code first to standardize the size of the images and then read all 87 of them from the data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setup a standard image size; this will distort some images but will get everything into the same shape\n",
    "STANDARD_SIZE = (322, 137)\n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    \"\"\"\n",
    "    takes a filename and turns it into a numpy array of RGB pixels\n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    if verbose==True:\n",
    "        print \"changing size from %s to %s\" % (str(img.size), str(STANDARD_SIZE))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    img = list(img.getdata())\n",
    "    img = map(list, img)\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def flatten_image(img):\n",
    "    \"\"\"\n",
    "    takes in an (m, n) numpy array and flattens it \n",
    "    into an array of shape (1, m * n)\n",
    "    \"\"\"\n",
    "    s = img.shape[0] * img.shape[1]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "checks_dir = \"../data/checks/\"\n",
    "dollars_dir = \"../data/dollars/\"\n",
    "def images(img_dir):\n",
    "    return [img_dir+f for f in os.listdir(img_dir)]\n",
    "checks=images(checks_dir)\n",
    "dollars=images(dollars_dir)\n",
    "images=checks+dollars\n",
    "labels = [\"check\" for i in range(len(checks))] + [\"dollar\" for i in range(len(dollars))]\n",
    "len(labels), len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what some of these images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    display(Im(checks[i]))\n",
    "for i in range(3):\n",
    "    display(Im(dollars[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What features might you use to distinguish the cash notes from the checks? Here is an example of transforming an image into its R channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i0=images[20]\n",
    "display(Im(i0))\n",
    "i0m=img_to_matrix(i0)\n",
    "print i0m.shape\n",
    "plt.imshow(i0m[:,1].reshape(137,322))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for every image, flattening each image into 3 channels of 44114 pixels, for a total of 132342 features per image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for image in images:\n",
    "    img = img_to_matrix(image)\n",
    "    img = flatten_image(img)\n",
    "    data.append(img)\n",
    "\n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.where(np.array(labels)==\"check\", 1, 0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now carryout a 20D PCA, which captures 73% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_pca(d,n):\n",
    "    pca = PCA(n_components=n)\n",
    "    X = pca.fit_transform(d)\n",
    "    print pca.explained_variance_ratio_\n",
    "    return X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X20, pca20=do_pca(data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(pca20.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for kicks, because we can plot it, we'll do the 2D PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2, pca2=do_pca(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\": X2[:, 0], \"y\": X2[:, 1], \"label\":np.where(y==1, \"check\", \"dollar\")})\n",
    "colors = [\"red\", \"yellow\"]\n",
    "for label, color in zip(df['label'].unique(), colors):\n",
    "    mask = df['label']==label\n",
    "    plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick visual shows that 2Dims may be enough to allow for linear separation of checks from dollars, with 42% of the variance accounted for. \n",
    "\n",
    "We provide some code below to reconstruct, from the principal components, the images corresponding to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normit(a):\n",
    "    a=(a - a.min())/(a.max() -a.min())\n",
    "    a=a*256\n",
    "    return np.round(a)\n",
    "\n",
    "def getRGB(o):\n",
    "    size=322*137*3\n",
    "    r=o[0:size:3]\n",
    "    g=o[1:size:3]\n",
    "    b=o[2:size:3]\n",
    "    r=normit(r)\n",
    "    g=normit(g)\n",
    "    b=normit(b)\n",
    "    return r,g,b\n",
    "\n",
    "def getNC(pc, j):\n",
    "    return getRGB(pc.components_[j])\n",
    "\n",
    "def getMean(pc):\n",
    "    m=pc.mean_\n",
    "    return getRGB(m)\n",
    "\n",
    "def display_from_RGB(r, g, b):\n",
    "    rgbArray = np.zeros((137,322,3), 'uint8')\n",
    "    rgbArray[..., 0] = r.reshape(137,322)\n",
    "    rgbArray[..., 1] = g.reshape(137,322)\n",
    "    rgbArray[..., 2] = b.reshape(137,322)\n",
    "    img = Image.fromarray(rgbArray)\n",
    "    plt.imshow(np.asarray(img))\n",
    "    ax=plt.gca()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax\n",
    "\n",
    "def display_component(pc, j):\n",
    "    r,g,b = getNC(pc,j)\n",
    "    return display_from_RGB(r,g,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use these to see the first two PC's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_component(pca2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It looks like the contrast difference between the presidential head and the surroundings is the main key to doing the classifying.  Let's see the second PC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_component(pca2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The second PC seems to capture general darkness of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Do a 5 dimensional PCA, get the variance explanation, and display the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_from_RGB(*getMean(pca5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Logistic Classifier with CV\n",
    "\n",
    "Recall for CV we do the train/test split not once but multiple times on a grid of possible values for the optimal parameter `C` and for each `C` we:\n",
    "\n",
    "1. create `n_folds` folds. Since the data size is approx. 90 here, and we have 5 folds, we roughly split the data into folds of 17-18 values each, randomly. \n",
    "2. We then train on 4 of these folds, test on the 5th\n",
    "3. We average the results of all such combinations\n",
    "4. We move on to the next value of `C`, and find the optimal value that minimizes mean square error.\n",
    "5. We finally use that value to make the final fit.\n",
    "\n",
    "We provide the following helper functions to do this. Notice the structure of the `GridSearchCV` estimator in `cv_optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_logistic(X_train, y_train, reg=0.0001, penalty=\"l2\"):\n",
    "    clf = LogisticRegression(C=reg, penalty=penalty)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def cv_optimize(X_train, y_train, paramslist, penalty=\"l2\", n_folds=10):\n",
    "    clf = LogisticRegression(penalty=penalty)\n",
    "    parameters = {\"C\": paramslist}\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(X_train, y_train)\n",
    "    return gs.best_params_, gs.best_score_\n",
    "\n",
    "def cv_and_fit(X_train, y_train, paramslist, penalty=\"l2\", n_folds=5):\n",
    "    bp, bs = cv_optimize(X_train, y_train, paramslist, penalty=penalty, n_folds=n_folds)\n",
    "    print \"BP,BS\", bp, bs\n",
    "    clf = fit_logistic(X_train, y_train, penalty=penalty, reg=bp['C'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a helper to show classification boundaries as well as distinguish test and training points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def points_plot(Xtr, Xte, ytr, yte, clf):\n",
    "    X=np.concatenate((Xtr, Xte))\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
    "                         np.linspace(y_min, y_max, 50))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    f,ax = plt.subplots()\n",
    "    # Plot the training points\n",
    "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(Xte[:, 0], Xte[:, 1], c=yte, cmap=cm_bright, marker=\"s\", s=50, alpha=0.9)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.4)\n",
    "    cs2 = ax.contour(xx, yy, Z, cmap=cm, alpha=.4)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way of doing a 70/30 train-test data split manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "train_x, train_y = data[is_train], y[is_train]\n",
    "test_x, test_y = data[is_train==False], y[is_train==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *fit (find PC's) and transform* the training data, and then use the PC's to transform the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "train_x = pca.fit_transform(train_x)\n",
    "test_x = pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do a cross-validated logistic regression and output the confusion matrix. Note the large value of the regularization factor. Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = cv_and_fit(train_x, train_y, np.logspace(-4, 3, num=100))\n",
    "pd.crosstab(test_y, logreg.predict(test_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg.coef_, logreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points_plot(train_x, test_x, train_y, test_y, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a L1 penalty instead of L2. this is strictly not a correct thing to do since PCA and L2 regularization are both rotationally invariant. However, lets see what happen to the co-efficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_l1=cv_and_fit(train_x, train_y, np.logspace(-4, 3, num=100), penalty=\"l1\")\n",
    "pd.crosstab(test_y, logreg_l1.predict(test_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print logreg_l1.coef_, logreg_l1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points_plot(train_x, test_x, train_y, test_y, logreg_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice L1 regularization supresses the intercept and reduces the importance of the second dimension. If one wants to minimize non-zero coefficients, one uses L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Carry out a 5 dimensional PCA and then a logistic regression with both L2 and L1 penalties. Create crosstabs (confusion matracies) and print co-efficents for both. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Beyond Linearity\n",
    "\n",
    "Linear models are relatively simple to describe and implement, and have advantages over other approaches in terms of interpretation and inference. However, standard linear regression can have significant limitations in terms of predictive power. This is because the linearity assumption is almost always an approximation, and sometimes a poor one.\n",
    "\n",
    "We saw in the previous lectures improvements to the linear model by reducing the complexity of the model and hence it's variance but this will only get us up to a point.\n",
    "\n",
    "We now relax the linearity assumption while still attempting to maintain as much interpretability as possible by looking at:\n",
    "\n",
    "* Polynomial regression: extra predictors, obtained by raising each of the original predictors to a power\n",
    "* Step functions: cut the range of a variable into $K$ distinct regions in order to produce a qualitative variable\n",
    "* Regression splines: an extension of the above two methods\n",
    "* Smoothing splines: use a smoothing penalty\n",
    "* Local regression: similar to splines but use overlapping regions\n",
    "* Generalized additive models: extend all these models to deal with many predictors\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "Let's use the `Wage` data and factorise the categorical data via dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wage_df = pd.read_csv(\"../data/Wage.csv\",index_col ='id')\n",
    "wage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wage_df[\"sex\"] = pd.factorize(wage_df[\"sex\"])[0]\n",
    "wage_df[\"maritl\"] = pd.factorize(wage_df[\"maritl\"])[0]\n",
    "wage_df[\"race\"] = pd.factorize(wage_df[\"race\"])[0]\n",
    "wage_df[\"education\"] = pd.factorize(wage_df[\"education\"])[0]\n",
    "wage_df[\"region\"] = pd.factorize(wage_df[\"region\"])[0]\n",
    "wage_df[\"health\"] = pd.factorize(wage_df[\"health\"])[0]\n",
    "wage_df[\"health_ins\"] = pd.factorize(wage_df[\"health_ins\"])[0]\n",
    "wage_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, it is unusual to use a power greater than 3 or 4 for a feature and fit a model for it. For higher powers the polynomial curve can become overly flexible and can take on some very strange shapes, especially at the boundaries.\n",
    "\n",
    "Let us think about fitting a degree-4 polynomial of `wage` as a function of `age`. Even though this is a linear regression model like any other, the individual coefficients are not of particular interest. Instead, we look at the entire fitted function.\n",
    "\n",
    "## Orthogonal Polynomial Regression\n",
    "\n",
    "We could of course construct new age features with increasing powers, but we have to keep in mind an issue that can pop-up in practice. Powers of say `age` could be correlated with one another and regression on correlated  predictors leads to unstable coefficients: the coefficients from an order-3 polynomial regression might change drastically when moving to an order-4 regression. Also if values in our feature column take large values then their powers will grow even larger leading to a poorly conditioned matrix when it comes to solving gradient descent. Larger values will also mean smaller coefficients leading to under-flow problems or coefficients which are hard to interpret.\n",
    "\n",
    "We can fix this by using *orthogonal polynomial basis*. This is just a change in the coordinate system so has no effect on the regression.\n",
    "\n",
    "The following helper `poly()` does just this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from http://davmre.github.io/python/2013/12/15/orthogonal_poly/\n",
    "def poly(x, degree = 1):\n",
    "    n = degree + 1\n",
    "    x = np.asarray(x).flatten()\n",
    "    if(degree >= len(np.unique(x))):\n",
    "        print(\"'degree' must be less than number of unique points\")\n",
    "        return\n",
    "    xbar = np.mean(x)\n",
    "    x = x - xbar\n",
    "    X = np.fliplr(np.vander(x, n))\n",
    "    q, r = np.linalg.qr(X)\n",
    "\n",
    "    z = np.diag(np.diag(r))\n",
    "    raw = np.dot(q, z)\n",
    "    \n",
    "    norm2 = np.sum(raw ** 2, axis=0)\n",
    "    alpha = (np.sum((raw ** 2) * np.reshape(x,(-1,1)), axis=0) / norm2 + xbar)[:degree]\n",
    "    Z = raw / np.sqrt(norm2)\n",
    "    return Z, norm2, alpha\n",
    "\n",
    "X = poly(wage_df[\"age\"].values, 4)[0]\n",
    "X[0:5, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = wage_df[\"wage\"].values\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "print \"Intercepts:\", reg.intercept_\n",
    "print \"Coefficients:\", reg.coef_\n",
    "ax = wage_df.plot(x=\"age\", y=\"wage\", style=\"o\")\n",
    "ax.set_ylabel(\"wage\")\n",
    "# The poly() method cannot return features for a single X value, \n",
    "# so we have to plot the raw predictions.\n",
    "age = wage_df[\"age\"].values\n",
    "ypred = reg.predict(X)\n",
    "polyline = np.poly1d(np.polyfit(age, ypred, 4))\n",
    "xs = range(int(np.min(age)), int(np.max(age)))\n",
    "ys = polyline(xs) \n",
    "ax.plot(xs, ys, 'r', linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Logistic Regression\n",
    "\n",
    "It seems like the wages are from two distinct populations: there appears to be a high earners group earning more than $250K per year as well as a low earners group. We can treat `wage` as a binary variable by splitting it into these two groups. Logistic regression can then be used to predict this binary response, using polynomial functions of `age` as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vander(wage_df[\"age\"],5)\n",
    "y = wage_df[\"wage\"].map(lambda w: 1 if w > 250 else 0).values\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X, y)\n",
    "print \"Intercepts:\", reg.intercept_\n",
    "print \"Coefficients:\", reg.coef_\n",
    "\n",
    "ypred = reg.predict(X)\n",
    "print \"MSE:\", mean_squared_error(y, ypred)\n",
    "\n",
    "xs = range(min(wage_df[\"age\"]), max(wage_df[\"age\"]))\n",
    "plt.plot(xs, reg.predict_proba(np.vander(xs,5)))\n",
    "plt.xlabel(\"age\")\n",
    "plt.ylabel(\"p(wage | age)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splines\n",
    "\n",
    "Now we discuss a flexible class of basis functions that extends upon the polynomial regression approach above.\n",
    "\n",
    "Instead of fitting a high-degree polynomial over the entire range of X, piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of $X$.\n",
    "\n",
    "E.g. a piecewise cubic polynomial works by fitting:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_1 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "where the coefficients $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ differ in different parts of the range of $X$. The points where the coefficients change are called knots.\n",
    "\n",
    "So a model with one knot at $c$ will need to fit two models one for $x_i < c$ and one for $x_i \\ge c$. Using more knots leads to a more flexible piecewise polynomial. The problem is that the model will be discontinuous and looks ridiculous!\n",
    "\n",
    "### Constraints\n",
    "\n",
    "To fix this problem we can fit under a constraint: the fitted curve must be continuous. We do this by asking that the  first and second derivatives of the piecewise polynomials are continuous at the knots. These models are known as splines but unfortunately, splines can have high variance at the outer range of the predictors that is, when $X$ takes on either a very small or very large value.\n",
    "\n",
    "\n",
    "### Choosing the Location of Knots\n",
    "\n",
    "The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. \n",
    "\n",
    "While this option can work well, in practice it is common to place knots in a uniform fashion. Another way is to knot location at the 25th, 50th, and 75th percentiles of $X$. \n",
    "\n",
    "Another option is to use Mean shift clustering to discover “blobs” in a smooth density of samples.  It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ges = wage_df[\"age\"].values\n",
    "wages = wage_df[\"wage\"].values\n",
    "X = np.vstack((ages, wages)).T\n",
    "\n",
    "# cluster points to find the knot location\n",
    "msc = MeanShift()\n",
    "msc.fit(X)\n",
    "knots = msc.cluster_centers_[:, 0]\n",
    "\n",
    "# fit a spline over the points\n",
    "spl = LSQUnivariateSpline(ages, wages, knots)\n",
    "xs = range(np.min(ages), np.max(ages))\n",
    "ys = spl(xs)\n",
    "\n",
    "# plot the points and the spline\n",
    "ax = wage_df.plot(x=\"age\", y=\"wage\", style=\"o\")\n",
    "ax.set_ylabel(\"wage\")\n",
    "ax.plot(xs, ys, 'r', linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Number of Knots\n",
    "\n",
    "By now this should seem familiar:  use cross-validation. With this method, we remove a portion of the data, fit a spline with a certain number of knots to the remaining data, and then use the spline to make predictions for the held-out portion. We repeat this process multiple times until each observation has been left out once, and then compute the overall cross-validated RSS. This procedure can be repeated for different numbers of knots and we choose the number with the lowest RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Splines\n",
    "\n",
    "We now introduce a somewhat different approach that also produces a spline. Recall in fitting a smooth curve to a set of data, what we really want to do is find some function, $g(x)$ with small RSS = $\\sum_{i=1}^{n} (y_i - g(x_i))^2$. But if we do not put a contraint on $g(x)$ then we can always make RSS = 0 by choosing a very flexible function that interpolates all of the $y_i$. Our model will overfit. What we really want is a function that makes RSS small but is also smooth.\n",
    "\n",
    "A natural approach is to find the function $g$ that minimises:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (y_i - g(x_i))^2 + \\lambda \\int g''(t)^2\\,\\mathrm{d}t,\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is non-negative tuning parameter. The function that minimises this is called a smoothing spline. The second derivative is a measure of roughness and we are forcing the function to be smooth with this penalty. When $\\lambda$ is huge we will get a straight line and when it is small the function will be very jumpy.\n",
    "\n",
    "### Choosing $\\lambda$\n",
    "\n",
    "It turns out that we can compute the LOOCV error very efficiently for smoothing splines and thus we choose the $\\lambda$ with the lowest LOOCV RSS error.\n",
    "\n",
    "## Local Regression\n",
    "\n",
    "Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point $x_0$ using only the nearby training observations.\n",
    "\n",
    "Formally local regression at $X=x_0$ involves:\n",
    "\n",
    "1. Gather the fraction $s=k/n$ of traning points whose $x_i$ are closest to $x_0$\n",
    "2. Assign a weight $K_{i0}=K(x_i,x_0)$ to each point in this neighbourhood such that points further away from $x_0$ have zero weight and the closer ones have a heigher weight. All but the nearest $k$ get weight zero.\n",
    "3. Fir a weighted least square regression of the $y_i$ on the $x_i$ using the above weights by finding the coefficients that minimise:\n",
    "$$\n",
    "\\sum_{i-1}^{n}K_{i0}(y_i - \\beta_0 -\\beta_1 x_i)^2.\n",
    "$$\n",
    "4. The model at $x_0$ is given by $\\hat{f}(x_0)=\\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$.\n",
    "\n",
    "This method however suffers from the curse of dimensionality and usually performs poorly for $p$ larger than 3 or 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
